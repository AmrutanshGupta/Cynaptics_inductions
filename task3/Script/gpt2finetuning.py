# -*- coding: utf-8 -*-
"""GPT2FINETUNING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K0u8UKaxt5uVNTXvr9B-AHJT2w-V8Rj7
"""

import numpy as np
import re
import pandas as pd

!pip install datasets
!pip install transformers

from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
from datasets import Dataset
from transformers import GPT2Tokenizer

from google.colab import drive
drive.mount('/content/drive')

def cleaning(s):
    s = str(s)
    s = re.sub(r'\s\W', ' ', s)
    s = re.sub(r'\W,\s', ' ', s)
    s = re.sub(r"\d+", "", s)
    s = re.sub(r'\s+', ' ', s)
    s = re.sub(r'[!@#$_]', '', s)
    s = s.replace("co", "")
    s = s.replace("https", "")
    s = s.replace("[\w*", " ")
    return s

from datasets import Dataset
from transformers import GPT2Tokenizer

def preprocess_persona_chat(data):
    """Preprocess the persona-chat dataset to create input-output pairs."""
    inputs = []
    outputs = []

    for idx, row in data.iterrows():
        # Clean persona, dialogue, and reference text
        persona = "Persona: " + " ".join([cleaning(p) for p in row["persona_b"]])
        dialogue = "Dialogue: " + " ".join([cleaning(d) for d in row["dialogue"]])
        reference = cleaning(row["reference"])

        # Combine persona and dialogue as input, with the reference as the output
        input_text = f"{persona}\n{dialogue}\n"
        inputs.append(input_text)
        outputs.append(reference)

    return pd.DataFrame({"input": inputs, "output": outputs})

# Load the dataset
df = pd.read_parquet("hf://datasets/Cynaptics/persona-chat/data/train-00000-of-00001.parquet")
processed_data = preprocess_persona_chat(df)

# Convert to Hugging Face dataset for tokenization
hf_dataset = Dataset.from_pandas(processed_data)

from transformers import GPT2LMHeadModel, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from datasets import DatasetDict

# Load tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Ensure PAD token is set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    # Tokenize input (persona + dialogue)
    model_inputs = tokenizer(
        examples["input"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

    # Tokenize reference as labels
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["output"],
            padding="max_length",
            truncation=True,
            max_length=512
        )

    # Add labels to the model inputs
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


tokenized_datasets = hf_dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["c_attn"]
)

# Load model and apply LoRA
model = GPT2LMHeadModel.from_pretrained(model_name)
model = get_peft_model(model, lora_config)

# Training arguments
training_args = TrainingArguments(
    output_dir="./lora_persona_gpt2",
    evaluation_strategy="epoch",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

# Fine-tune the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained("./lora_persona_gpt2")
tokenizer.save_pretrained("./lora_persona_gpt2")

from google.colab import drive
drive.mount('/content/drive')
output_dir = '/content/drive/MyDrive/fine_tuned_persona_gpt2'
from transformers import Trainer

# Assume trainer is already defined
trainer.save_model(output_dir)

# Save the tokenizer explicitly
tokenizer.save_pretrained(output_dir)

# Save model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

from transformers import GPT2LMHeadModel, GPT2Tokenizer

def generate_response(prompt, model_path, max_length=100):
    """Generate a response based on the input prompt."""
    model = GPT2LMHeadModel.from_pretrained(model_path)
    tokenizer = GPT2Tokenizer.from_pretrained(model_path)

    # Tokenize input
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).input_ids

    # Generate response
    outputs = model.generate(
        input_ids=input_ids,
        max_length=max_length,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        pad_token_id=tokenizer.pad_token_id,
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example prompt
prompt = """
Person B has the following Persona information.

Persona of Person B: My name is David and I'm a 35 year old math teacher.
Persona of Person B: I like to hike and spend time in the nature.
Persona of Person B: I'm married with two kids.

Instruct: Person A and Person B are now having a conversation.
Following the conversation below, write a response that Person B would say base on the
above Persona information.
Please carefully consider the flow and context of the conversation below, and use the Person B's Persona information appropriately to generate a response that you think are
the most appropriate replying for Person B.

Persona A: Morning! I think I saw you at the parent meeting, what's your name?

Output:
"""

# Generate response
response = generate_response(prompt, "./lora_persona_gpt2")
print("Generated Response:", response)